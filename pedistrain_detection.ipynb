{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOimqyioHux+C2NGy4EdH3Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MdA-Saad/MaskRCNN-Pedestrian-Detection-and-Segmentation/blob/main/pedistrain_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.io import read_image\n",
        "from torchvision.ops.boxes import masks_to_boxes\n",
        "from torchvision import tv_tensors\n",
        "from torchvision.transforms.v2 import functional as tF\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks"
      ],
      "metadata": {
        "id": "86LQmMajOvk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhhGvIQiHdyq",
        "outputId": "5030b7c6-b6b6-483d-8196-3ceafa8360c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_folder='/content/drive/My Drive/Colab Notebooks/projects/pennFudan-Peds/'\n",
        "os.listdir(path_to_folder)"
      ],
      "metadata": {
        "id": "acEWQm5qH8Yd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d74f712-87f3-462c-9417-6adb654db27e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pennFudandDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, root, transform):\n",
        "    self.root = root\n",
        "    self.transform = transform\n",
        "    self.imgs=list(sorted(os.listdir(os.path.join(root,\"PNGImages\"))))\n",
        "    self.masks=list(sorted(os.listdir(os.path.join(root,\"PedMasks\"))))\\\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    img_path=os.path.join(self.root,\"PNGImages\",self.imgs[idx])\n",
        "    mask_path=os.path.join(self.root,\"PedMasks\",self.masks[idx])\n",
        "    img=read_image(img_path)\n",
        "    mask=read_image(mask_path)\n",
        "    obj_ids=torch.unique(mask) #this is a vector of 1 dim\n",
        "    obj_ids=obj_ids[1:] #obj_ids[0] is background and we dont want it.\n",
        "    num_objs=len(obj_ids) #number of pedestrains\n",
        "\n",
        "    masks=(mask==obj_ids[:,None,None]).to(dtype=torch.uint8)\n",
        "    \"\"\"both mask and obj_ids are broadcasted. Mask is broadcasted along dim 0\n",
        "    while obj_ids is broadcasted along dim 1 and 2.\n",
        "    \"\"\"\n",
        "\n",
        "    boxes=masks_to_boxes(masks)\n",
        "    label=torch.ones((num_obj),dtype=torch.int64) # 1 represents the pedestrain\n",
        "    #Model expects int64 for the labels\n",
        "\n",
        "    image_id=idx\n",
        "    area=(boxes[:,3]-boxes[:,1])*(boxes[:,2]-boxes[:,0])\n",
        "    iscrowd=torch.zeros((num_objs),dtype=torch.int64)\n",
        "    img=tv_tensors.Image(img)\n",
        "\n",
        "    target={}\n",
        "    target[\"boxe\"]=tv_tensors.BoundingBoxes(boxes,format=\"XYXY\",canvas_size=tF.get_size(img))\n",
        "    target[\"labels\"]=labels\n",
        "    target[\"image_id\"]=image_id\n",
        "    target[\"area\"]=area\n",
        "    target[\"iscrowd\"]=iscrowd\n",
        "\n",
        "    if self.transform is not None:\n",
        "      img,target=self.transfor(img,target)\n",
        "    return img,target\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.imgs)\n"
      ],
      "metadata": {
        "id": "G6cwirynO2la"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "  return tuple(zip(*batch))\n",
        "\n",
        "def get_transform(train):\n",
        "  transform=[]\n",
        "  if train:\n",
        "    transform.append(tF.RandomHorizontalFlip(0.5))\n",
        "  transforms.append(tF.ToDtype(torch.float32,scale=True))\n",
        "  transforms.append(tF.ToPureTensor())\n",
        "  return tF.compose(transforms)"
      ],
      "metadata": {
        "id": "vOEID_D52was"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_instance_segementation():\n",
        "  num_classes=2\n",
        "  model=torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
        "  in_features=model.roi_heads.box_predictor.cls_score.in_features\n",
        "  model.roi_heads.box_predictor=FastRCNNPredictor(in_features,num_classes)\n",
        "  in_features_mask=model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "  hidden_layer=256\n",
        "  model.roi_heads.mask_predictor=MaskRCNNPredictor(\n",
        "      in_features_mask,\n",
        "      hidden_layer,\n",
        "      num_classes\n",
        "  )\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "iZURtSZ-x3I8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model,optimizer,data_loader,device):\n",
        "  model.train()\n",
        "  for images,targets in data_loader:\n",
        "    images=list(image.to(device) for image in images)\n",
        "    targets=[{k:v.to(device) for k,v in t.items()} for t in targets]\n",
        "    loss_dict=model(images,targets)\n",
        "    losses=sum(loss for loss in loss_dict.values())\n",
        "    optimizer.zero_grad()\n",
        "    losses.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f\"loss:{losses.item()}\")"
      ],
      "metadata": {
        "id": "YRRHaEn96oLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.inference_mode()\n",
        "def evaluate(model,data_loader,device):\n",
        "  model.eval()\n",
        "  results=[]\n",
        "\n",
        "  for images,targets in data_loader:\n",
        "    images=list(image.to(device) for img in images)\n",
        "    outputs=model(images)\n",
        "    outputs=[{k:v.to(\"cpu\") for k,v in t.items()} for t in outputs]\n",
        "    results.append(outputs)\n",
        "  print(f\"Inference finished on {len(data_loader.dataset)} images.\")\n",
        "  return results"
      ],
      "metadata": {
        "id": "c2EDttqDE4BZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device=torch.accelerator.current_accelerator() if torch.accelerator.is_available() else torch.device('cpu')\n",
        "\n",
        "dataset=PennFudanDataset('data/PennFudan',get_transform(train=True))\n",
        "dataset_test=PennFudanDataset('data/PennFudan',get_transform(train=False))\n",
        "\n",
        "indices=torch.randperm(len(dataset)).tolist()\n",
        "dataset=torch.utils.data.Subset(dataset,indices[:-50])\n",
        "dataset_test=torch.utils.data.Subset(dataset_test,indices[-50:])\n",
        "\n",
        "data_loader=torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=2,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "data_loader_test=torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "model=get_model_instance_segementation()\n",
        "model.to(device)\n",
        "\n",
        "params=[p for p in model.parameters() if p.requires_grad]\n",
        "optimizer=torch.optim.SGD(\n",
        "    params,\n",
        "    lr=0.005,\n",
        "    momentum=0.9,\n",
        "    weight_decay=0.0005\n",
        ")\n",
        "\n",
        "lr_scheduler=torch.optim.lr_scheduler.StepLR(\n",
        "    optimizer,\n",
        "    step_size=3,\n",
        "    gamma=0.1\n",
        ")\n",
        "num_epochs=1\n",
        "for epoch in range(num_epochs):\n",
        "  train(model,optimizer,data_loader,device)\n",
        "  lr_scheduler.step()\n",
        "  evaluate(model,data_loader_test,device=device)\n",
        "print(\"That's it\")\n"
      ],
      "metadata": {
        "id": "QGwsrwr2Aulm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}